# Node Map LLM UI

A visual conversation tree interface for interacting with Large Language Models. This application displays LLM conversations as an interactive node graph, enabling branching conversations and merging multiple conversation paths.

![Node Map LLM UI Screenshot](screenshot.png)

## Features

- **Visual Conversation Tree**: See your entire conversation history as an interactive directed acyclic graph (DAG)
- **Branching Conversations**: Create alternative responses by branching from any point in the conversation
- **Edit and Branch**: Double-click any user message to edit it inline
  - Updates in place if the node has no descendants
  - Creates a new variation branch if descendants exist, preserving the original conversation path
- **Merge Nodes**: Combine multiple conversation branches into a single context for the LLM
- **Context Management**: Real-time context window tracking with visual indicators and warnings
  - Token usage visualization on nodes and in sidebar
  - Custom summarization with dialog-based guidance and review
  - Manual node deletion and pruning tools
  - Per-branch context percentage indicators
- **Multiple Chat Management**: Create, switch between, and delete separate conversation sessions
- **Streaming Responses**: Real-time streaming of LLM responses with abort capability
- **Auto-generated Summaries**: Branch summaries generated by the LLM for quick context
- **Web Search Integration**: Optional Searxng integration for LLM web search capability
- **Document Upload & RAG**: Upload documents (PDF, DOCX, XLSX, TXT, images) for context-aware conversations
  - Node-scoped document association with branch isolation
  - Automatic text extraction, chunking, and embedding generation
  - Vector similarity search for retrieving relevant context
  - Visual document indicators (green ðŸ“Ž badges) on nodes with attachments
  - Automatic cleanup when nodes or chats are deleted
- **Configurable Embeddings**: Support for any OpenAI-compatible embedding API
  - Custom endpoint and model configuration
  - Adjustable chunk size and search parameters
  - RAG token tracking included in context calculations
- **Resizable Sidebar**: Adjustable chat panel width with persistence
- **Dark Theme**: Modern dark UI optimized for extended use
- **Performance Optimized**: Token caching, memoized calculations, and debounced updates

## Tech Stack

- **React 18** - UI framework
- **TypeScript** - Type safety
- **Vite** - Build tool and dev server
- **Zustand** - State management with persistence
- **React Flow (@xyflow/react)** - Interactive node graph visualization
- **Dagre** - Automatic graph layout
- **Tailwind CSS v4** - Styling
- **React Markdown** - Message rendering with GFM support

## Requirements

- Node.js 18+
- npm or yarn
- Desktop browser (minimum 1024px viewport width)
- OpenAI-compatible API endpoint

## Installation

```bash
# Clone the repository
git clone <repository-url>
cd node-map-llm-ui

# Install dependencies
npm install

# Start development server
npm run dev
```

## Configuration

On first launch, click the settings icon to configure your LLM connection:

1. **API Endpoint**: Your OpenAI-compatible API URL (e.g., `https://api.openai.com/v1`)
2. **API Key**: Your API key for authentication
3. **Model**: Select from available models or enter a custom model name

The application supports any OpenAI-compatible API including:
- OpenAI
- Ollama (local)
- LM Studio (local)
- Azure OpenAI
- Other compatible providers

### Web Search (Optional)

Enable web search to let the LLM use search results for current information:

1. Set up a [Searxng](https://github.com/searxng/searxng) instance (self-hosted)
2. Configure the `SEARXNG_ENDPOINT` environment variable on the server:
   ```bash
   # For local development
   SEARXNG_ENDPOINT=http://localhost:8080 npm run dev:server

   # For Docker (use host.docker.internal to reach host machine)
   SEARXNG_ENDPOINT=http://host.docker.internal:8080
   ```
3. In the app Settings, enable "Web Search" and click "Test Connection" to verify
4. Use the "Search" toggle button next to the input field to enable search per-message

When search is enabled for a message, results are automatically injected into the conversation context. Nodes that used search display a blue search icon, and sources appear below assistant messages in the chat sidebar.

### Document Upload & RAG (Retrieval-Augmented Generation)

Upload documents to provide the LLM with additional context from your files:

1. **Configure Embeddings** (Settings â†’ Embeddings):
   - Set your embedding API endpoint (OpenAI-compatible)
   - Enter API key if required
   - Select embedding model (e.g., `text-embedding-3-small`, `text-embedding-ada-002`)
   - Adjust chunk size (default: 512 tokens) and search parameters

2. **Upload Documents**:
   - Click the ðŸ“Ž button in the chat sidebar
   - Select files: PDF, DOCX, XLSX, PPTX, TXT, MD, or images (PNG, JPG)
   - Files are automatically processed: text extracted, chunked, and embedded
   - Green ðŸ“Ž badge appears on nodes with attached documents

3. **Document Scoping**:
   - Documents attach to the current conversation node (usually a user message)
   - Documents are accessible by that node and all its descendants
   - Branch isolation: Documents don't pollute alternative conversation paths
   - Navigate to a node to see which documents are available in that context

4. **Automatic RAG**:
   - When RAG is enabled (Settings â†’ RAG), document search happens automatically
   - User's message is used to find relevant chunks via vector similarity
   - Top matching chunks are injected into the LLM context
   - RAG tokens are tracked and included in context percentage calculations

5. **Document Management**:
   - View attached documents in the sidebar (expandable list)
   - Delete documents individually if no longer needed
   - Documents automatically cleaned up when nodes or chats are deleted

**Supported File Types:**
- **Text**: `.txt`, `.md`
- **Documents**: `.pdf`, `.docx`, `.xlsx`, `.pptx`
- **Images**: `.png`, `.jpg` (OCR/vision extraction)

## Usage

### Basic Conversation
1. Type a message in the input field at the bottom of the sidebar
2. Press Enter or click Send to submit
3. The conversation appears as nodes in the canvas view

### Branching
- Click any node in the conversation tree to navigate to that point
- Send a new message to create a branch from the selected node
- Multiple branches can exist from any single node

### Edit and Branch
- Double-click any user message node (purple nodes) to edit it
- If the node has no responses below it, the edit updates in place
- If the node has responses below it:
  - The "Branch & Regenerate" button creates a new variation branch
  - The original conversation path remains intact
  - A new AI response is automatically generated for the edited prompt
- Press Escape to cancel editing without changes

### Merging Branches
1. Hold Shift and click nodes to select multiple conversation endpoints
2. Click the "Merge" button that appears
3. The LLM receives context from all selected branches combined
4. A merge node is created showing the consolidated conversation

### Navigation
- **Pan**: Click and drag on the canvas
- **Zoom**: Mouse wheel or pinch gesture
- **Select Node**: Click a node to navigate to that point in the conversation
- **Fit View**: Double-click empty canvas area

### Context Management
The app tracks context window usage to prevent exceeding model limits:

- **Context Indicator**: Shows in sidebar when approaching 60% of model's context limit
  - Green: 0-79% (normal)
  - Yellow: 80-94% (warning)
  - Red: 95%+ (critical)
- **Node Badges**: Context percentage appears on canvas nodes when above 60%
- **Custom Summarization**: Right-click any node and select "Summarize up to here" to compress older messages
  - **Prompt Dialog**: Customize what aspects are most important to preserve (key decisions, technical details, action items, etc.)
  - **Review Dialog**: Edit the generated summary or retry with different guidance
  - **Prompt Memory**: Your summarization preferences are remembered per conversation
  - User guidance is combined with comprehensive system instructions for optimal results
- **Node Deletion**: Right-click nodes to delete them (with confirmation for nodes with children)
- **Auto-detection**: Model context limits are auto-detected for common models (Llama, Mistral, etc.)

## Project Structure

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Canvas/          # React Flow canvas view
â”‚   â”œâ”€â”€ ChatSidebar/     # Chat interface and message display
â”‚   â”œâ”€â”€ Chats/           # Multi-chat management modal
â”‚   â”œâ”€â”€ DocumentUpload/  # Document upload component with drag & drop
â”‚   â””â”€â”€ Settings/        # Configuration modal (LLM, Embeddings, RAG)
â”œâ”€â”€ edges/               # Custom React Flow edge types
â”œâ”€â”€ nodes/               # Custom React Flow node types
â”‚   â”œâ”€â”€ ConversationNode # Message nodes with document indicators
â”‚   â”œâ”€â”€ MergeNode        # Branch merge nodes
â”‚   â””â”€â”€ SummaryNode      # Summarized content nodes
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llmService.ts    # LLM API with RAG integration
â”‚   â”œâ”€â”€ apiService.ts    # Backend API client
â”‚   â”œâ”€â”€ contextService.ts # Token counting with RAG tokens
â”‚   â””â”€â”€ searchService.ts # Web search integration
â”œâ”€â”€ store/
â”‚   â”œâ”€â”€ conversationStore.ts # Main state with document management
â”‚   â””â”€â”€ settingsStore.ts     # Config persistence (LLM, Embeddings, RAG)
â”œâ”€â”€ types/               # TypeScript type definitions
â””â”€â”€ utils/               # Layout and helper utilities

server/
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ chats.ts         # Chat and node endpoints with cleanup
â”‚   â””â”€â”€ documents.ts     # Document upload, search, management
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ documentProcessor.ts  # Text extraction orchestrator
â”‚   â”œâ”€â”€ embeddingService.ts   # Embedding generation
â”‚   â”œâ”€â”€ vectorSearchService.ts # Cosine similarity search
â”‚   â”œâ”€â”€ chunkingService.ts    # Text chunking
â”‚   â””â”€â”€ extractors/           # Format-specific extractors
â”‚       â”œâ”€â”€ pdfExtractor.ts
â”‚       â”œâ”€â”€ docxExtractor.ts
â”‚       â”œâ”€â”€ xlsxExtractor.ts
â”‚       â””â”€â”€ textExtractor.ts
â””â”€â”€ db/
    â””â”€â”€ index.ts         # SQLite with document tables
```

## Scripts

```bash
npm run dev      # Start development server
npm run build    # Build for production
npm run preview  # Preview production build
npm run lint     # Run ESLint
```

## Docker

Build and run with Docker Compose:

```bash
# Build and start
docker-compose up --build -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

The application will be available at `http://localhost:3002`.

Alternatively, build and run with Docker directly:

```bash
# Build the image
docker build -t node-map-llm-ui .

# Run the container
docker run -p 3002:3000 node-map-llm-ui
```

## License

MIT
